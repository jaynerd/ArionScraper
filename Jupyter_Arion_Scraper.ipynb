{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import re\n",
    "import json\n",
    "import urllib3\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialization\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "http = urllib3.PoolManager()\n",
    "target_urls = []\n",
    "scrapped_urls = []\n",
    "title = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arion scraper attributes\n",
    "arion_base_url = 'https://arion.aut.ac.nz/ArionMain/CourseInfo/Information/Qualifications/'\n",
    "arion_initial_url = 'QualificationTypes.aspx'\n",
    "target_urls.append(arion_initial_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arion scrapper methods\n",
    "\n",
    "# scrap nested urls\n",
    "def scrap_arion_urls():\n",
    "    for url in target_urls:\n",
    "        request = http.request('GET', arion_base_url + url)\n",
    "        soup = BeautifulSoup(request.data, 'lxml')\n",
    "        for element in soup.find_all('a', 'Navigation'):\n",
    "            scrapped_urls.append(element)\n",
    "\n",
    "# scrap nested entries into a list\n",
    "def scrap_simple_entries():\n",
    "    target_entries = []\n",
    "    for tag in scrapped_urls:\n",
    "        target_entries.append(tag.text)\n",
    "    return target_entries\n",
    "    \n",
    "# extract entries from scrapped urls, return a dictionary\n",
    "def get_arion_entries(dictionary_name: str):\n",
    "    dictionary = {}\n",
    "    dictionary.setdefault(dictionary_name, {})\n",
    "    previous_degree: str\n",
    "    previous_degree = ''\n",
    "    counter = 0\n",
    "    for tag in scrapped_urls:\n",
    "        if tag.text == previous_degree:\n",
    "            counter += 1\n",
    "            temp_text = tag.text + ' ' + str(counter)\n",
    "            dictionary[dictionary_name].setdefault(temp_text, {})\n",
    "        else:\n",
    "            dictionary[dictionary_name].setdefault(tag.text, {})\n",
    "            previous_degree = tag.text\n",
    "    return dictionary\n",
    "\n",
    "# extract paper entries for creating a paper list only\n",
    "def get_arion_paper_list():\n",
    "    dictionary = {}\n",
    "    target_entries = []\n",
    "    return_text = 'Returning to Qualification Details'\n",
    "    for tag in scrapped_urls:\n",
    "        target_entries.append(tag.text)\n",
    "    while return_text in target_entries:\n",
    "        target_entries.remove(return_text)\n",
    "    counter = 0\n",
    "    for entry in target_entries:\n",
    "        if counter % 2 is 0:\n",
    "            dictionary.setdefault(entry, [])\n",
    "        else:\n",
    "            dictionary[target_entries[(counter - 1)]] = entry\n",
    "        counter += 1\n",
    "    paper_list = []\n",
    "    for key in dictionary:\n",
    "        paper_list.append({'value': key, 'synonyms': [key, dictionary[key], re.sub('[^A-Z]', '', dictionary[key])]})\n",
    "    return paper_list\n",
    "\n",
    "# extract a set of papers for each degress\n",
    "def get_arion_course_set(dictionary: dict):\n",
    "    counter = 0\n",
    "    for degree in dictionary['qualifications']:\n",
    "        scrapped_course_urls = []\n",
    "        request = http.request('GET', arion_base_url + target_urls[counter])\n",
    "        soup = BeautifulSoup(request.data, 'lxml')\n",
    "        for element in soup.find_all('a', 'Navigation'):\n",
    "            scrapped_course_urls.append(element)\n",
    "        target_papers = []\n",
    "        for tag in scrapped_course_urls:\n",
    "            target_papers.append(tag.text)\n",
    "        return_text = 'Returning to Qualification Details'\n",
    "        while return_text in target_papers:\n",
    "            target_papers.remove(return_text)\n",
    "        iterator = 0\n",
    "        temp_paper_code: str\n",
    "        for paper in target_papers:\n",
    "            if iterator % 2 is 0:\n",
    "                dictionary['qualifications'][degree].setdefault(paper, [])\n",
    "                temp_paper_code = paper\n",
    "            else:\n",
    "                dictionary['qualifications'][degree][temp_paper_code].append({'name': paper})\n",
    "            iterator += 1\n",
    "        counter += 1\n",
    "    return dictionary\n",
    "\n",
    "# extract requisite information for every existing courses at AUT\n",
    "def get_requisites(dictionary: dict):\n",
    "    for url in target_urls:\n",
    "        request = http.request('GET', arion_base_url + url)\n",
    "        soup = BeautifulSoup(request.data, \"lxml\")\n",
    "        title = soup.find('td', {'width': '150'})\n",
    "        if title is not None:\n",
    "            title = title.text.strip()\n",
    "            guide_urls = soup.find_all('table', id = re.compile('^wucControl_repQualifications__ctl1_wucPaperRequisites'))\n",
    "            requisite_urls = soup.find_all('a', id = re.compile('^wucControl_repQualifications__ctl1_wucPaperRequisites'))\n",
    "            for url in guide_urls:\n",
    "                requisite_type = url.find_next('td')\n",
    "                dictionary['papers'][title].setdefault(requisite_type.text, [])\n",
    "                for tag in requisite_urls:\n",
    "                    temp_list = dictionary['papers'][title][requisite_type.text]\n",
    "                    if tag.text not in temp_list:\n",
    "                        dictionary['papers'][title][requisite_type.text].append(tag.text)\n",
    "                        nextTag = tag\n",
    "                        requisite_urls.remove(tag)\n",
    "                        if nextTag.name != 'a':\n",
    "                            break\n",
    "            #print(title)\n",
    "            #print(dictionary['papers'][title])\n",
    "    return dictionary\n",
    "                    \n",
    "# sort paper entries in the right order of the paper code and its name\n",
    "def sort_entries(entries: list):\n",
    "    counter = 0\n",
    "    dictionary = {}\n",
    "    dictionary['papers'] = {}\n",
    "    for i in range(0, int(len(entries) / 2)):\n",
    "        dictionary['papers'].setdefault(entries[counter], {})\n",
    "        temp_list = dictionary['papers'][entries[counter]]\n",
    "        if{'name': entries[counter + 1]} not in temp_list.values():\n",
    "            dictionary['papers'][entries[counter]] = {'name': entries[counter + 1]}\n",
    "        counter += 2\n",
    "    return dictionary\n",
    "\n",
    "# add scrapped urls to target urls for the next round, then clear\n",
    "def clean_up(is_refined: bool):\n",
    "    target_urls.clear()\n",
    "    for url in scrapped_urls:\n",
    "        url = url.get('href')\n",
    "        if not is_refined:\n",
    "            url = url.replace('../', '')\n",
    "        target_urls.append(url)\n",
    "    scrapped_urls.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arion scrapping process_01 -> getting qualification type links\n",
    "scrap_arion_urls()\n",
    "clean_up(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# arion scrapping process_02 -> getting qualification liks\n",
    "scrap_arion_urls()\n",
    "qualification_dict = get_arion_entries('qualifications')\n",
    "clean_up(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arion scrapping process_03 -> getting table of papers links\n",
    "scrap_arion_urls()\n",
    "clean_up(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arion scrapping process_04 -> gettting degrees with a set of papers\n",
    "qualification_dict = get_arion_course_set(qualification_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arion scrapping process_05 -> getting paper links\n",
    "scrap_arion_urls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arion scrapping process_06 -> getting a list of papers to parse in json for Rudy's paper list entity\n",
    "paper_list = get_arion_paper_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arion scrapping process_07 -> getting a list, not dict of papers to be sorted with requisites later on\n",
    "papers = scrap_simple_entries()\n",
    "return_text = 'Returning to Qualification Details'\n",
    "while return_text in papers:\n",
    "    papers.remove(return_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arion scrapping process_08 -> making a dictionary of papers only to store requisites and avoid duplicates\n",
    "paper_dict = sort_entries(papers)\n",
    "clean_up(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arion scrapping process_09 -> scrapping requisite properties from arion to make a complete list\n",
    "paper_dict = get_requisites(paper_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a degree list with a corresponding set of papers to a json file for firebase\n",
    "with open('degrees.json', 'w') as outfile:\n",
    "    json.dump(qualification_dict, outfile, indent = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a paper list as a json file for dialogflow\n",
    "with open('papers.json', 'w') as outfile:\n",
    "    json.dump(paper_list, outfile, indent = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a paper list with requisite information for firebase\n",
    "with open('requisites.json', 'w') as outfile:\n",
    "    json.dump(paper_dict, outfile, indent = 4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
